<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling</h1>
          <h4 class="subtitle is-5"><em>NeurIPS 2024 </em> </h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a >Quanwei Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://guanjz20.github.io/">Jiazhi Guan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=2Pedf3EAAAAJ">Kaisiyuan Wang</a><sup>3*</sup>,
            </span>
            <span class="author-block">
              <a href="http://home.ustc.edu.cn/~yuly/">Lingyun Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1Ae0CMgAAAAJ&hl=en">Wenqing Chu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a >Zhiqiang Feng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=pnuQ5UsAAAAJ&view_op=list_works&sortby=pubdate">Haocheng Feng</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1wzEtxcAAAAJ">Errui Ding</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://jingdongwang2017.github.io/">Jingdong Wang</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://faculty.ustc.edu.cn/xiehongtao/zh_CN/index.htm">Hongtao Xie</a><sup>1*</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>Department of Computer Vision Technology (VIS), Baidu Inc.</span>
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">

      <img src="./static/images/pipeline_seth2.png"  width="800px" height="500px" 
        type="application/pdf">
      </img>
      
      <h2 class="subtitle has-text-centered">
        The overview of our proposed framework ShowMaker. 
      </h2>

  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <source src="./static/videos/steve.mp4" type="video/mp4">
      </div>
    </div>
  </div>
</section> 
    





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Although significant progress has been made in human video generation, most previous studies focus on either human facial animation or full-body animation,
          which cannot be directly applied to produce realistic conversational human videos with frequent hand gestures and various facial movements simultaneously. 
          </p>
          <p>
          To address these limitations, we propose a 2D human video generation framework, named ShowMaker, capable of generating high-fidelity half-body conversational
          videos based on 2D key points via fine-grained diffusion modeling. 
          We leverage dual-stream diffusion models as the backbone of our framework and carefully design two novel components for crucial local regions (i.e., hands and face) that can
          be easily integrated into our backbone. 
          Specifically, to handle the challenging hand generation caused by sparse motion guidance, we propose a novel Key Point-based Fine-grained Hand Modeling module by amplifying positional information from
          raw hand key points and constructing a corresponding key point-based codebook.
          Moreover, to restore richer facial details in generated results, we introduce a Face Recapture module, which extracts facial texture features and global identity features
          from the aligned human face and integrates them into the diffusion process for face enhancement. 
          </p>
          <p>
          Extensive quantitative and qualitative experiments demonstrate the superior visual quality and temporal consistency of our method
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <!-- <div class="publication-video"> -->
          <!-- <video poster="" id="paper-video" autoplay controls muted loop playsinline height="100%"> -->
          <!-- <video width="800" height="600" controls>
            <source src="https://www.youtube.com/watch?v=jWvAglDGWxA&t=1s" type="video/mp4">
          </video> -->
          <iframe width="800" height="480"
           src="https://www.youtube.com/embed/Uw8CUv5d_YY?si=J7IG9QdlJF0e4wWy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
          </iframe>
          <!-- </video> -->
        <!-- </div> -->
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<!-- <video width="320" height="240" controls>
  <source src="videos/movie.mp4" type="video/new_sup_final_crf20.mp4">
  Your browser does not support the video tag.
</video> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yang2024showmaker,
  author    = {Quanwei Yang, Jiazhi Guan, Kaisiyuan Wang, Lingyun Yu, Wenqing Chu, Hang Zhou, Zhiqiang Feng, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie.},
  title     = {ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling},
  journal   = {NeurIPS},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            <!-- This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
